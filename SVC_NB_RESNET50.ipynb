{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHgIhBSRimUD",
        "outputId": "a9f812e8-d723-4c2c-a7a0-12e13fa2f3a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Path exists, conducting extraction of image data for processing. I decide to go the path of Gaussian NB using Historgrams\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/banana added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/balloon added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/backpack added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/blocks added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/apple added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/bag added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/blanket added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/bib added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/ball added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/bed added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/carpet added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/clock added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/bottle added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/brush added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/book added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/chair added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/box added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/booklight added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/bowl added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/broom added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/crayons added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/coat added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/comb added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/doll added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/drawer added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/curtain added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/cup added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/crib added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/door added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/diaper added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/juice added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/dustpan added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/high_chair added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/floor_mat added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/hat added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/fan added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/hanger added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/gloves added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/fork added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/fridge_magnet added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/laundry_basket added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/light added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/keys added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/mirror added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/measuring_cup added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/lamp added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/marker added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/labels added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/mobile added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/milk added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/plate added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/phone added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/pen added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/plastic_cup added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/pacifier added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/pajamas added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/notebook added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/pillow added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/nightlight added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/pants added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/playpen added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/puzzle added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/shirt added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/potty added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/scarf added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/rattle added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/rocking_chair added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/shampoo added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/remote added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/rubber_duck added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/shoes added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/sock added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/teddy_bear added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/spoon added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/sofa added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/table added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/stuffed_animal added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/sippy_cup added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/soap added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/tissue_box added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toothbrush added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_car added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_ring_stack added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toothpaste added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/towel added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_dinosaur added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_blocks added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_phone added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toilet_paper added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/watering_can added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_train added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/toy_xylophone added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/vacuum added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/water added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/trash_can added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/window added to processing queue\n",
            "/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset/train added to processing queue\n",
            "Total classes: 97\n",
            "Total images: 6976\n",
            "Classes found: ['coat', 'gloves', 'hanger', 'mirror', 'toy_ring_stack', 'spoon', 'juice', 'door', 'pen', 'water', 'rubber_duck', 'curtain', 'banana', 'backpack', 'sock', 'soap', 'bowl', 'rattle', 'cup', 'nightlight', 'remote', 'floor_mat', 'pants', 'toy_xylophone', 'booklight', 'bib', 'toy_train', 'potty', 'mobile', 'plate', 'fork', 'fan', 'sofa', 'notebook', 'hat', 'bag', 'watering_can', 'broom', 'towel', 'apple', 'doll', 'vacuum', 'pillow', 'plastic_cup', 'sippy_cup', 'light', 'tissue_box', 'ball', 'marker', 'box', 'crib', 'window', 'keys', 'blanket', 'toy_phone', 'fridge_magnet', 'teddy_bear', 'lamp', 'milk', 'comb', 'trash_can', 'toothpaste', 'stuffed_animal', 'toilet_paper', 'puzzle', 'phone', 'brush', 'shirt', 'pacifier', 'diaper', 'toothbrush', 'pajamas', 'train', 'high_chair', 'chair', 'shampoo', 'bed', 'rocking_chair', 'blocks', 'clock', 'shoes', 'measuring_cup', 'table', 'toy_dinosaur', 'dustpan', 'toy_car', 'playpen', 'crayons', 'scarf', 'bottle', 'book', 'toy', 'drawer', 'carpet', 'toy_blocks', 'laundry_basket', 'balloon']\n",
            "coat: 59 images\n",
            "gloves: 81 images\n",
            "hanger: 1 images\n",
            "mirror: 69 images\n",
            "toy_ring_stack: 78 images\n",
            "spoon: 80 images\n",
            "juice: 80 images\n",
            "door: 72 images\n",
            "pen: 71 images\n",
            "water: 71 images\n",
            "rubber_duck: 77 images\n",
            "curtain: 69 images\n",
            "banana: 73 images\n",
            "backpack: 73 images\n",
            "sock: 80 images\n",
            "soap: 85 images\n",
            "bowl: 83 images\n",
            "rattle: 72 images\n",
            "cup: 75 images\n",
            "nightlight: 67 images\n",
            "remote: 65 images\n",
            "floor_mat: 100 images\n",
            "pants: 69 images\n",
            "toy_xylophone: 1 images\n",
            "booklight: 61 images\n",
            "bib: 70 images\n",
            "toy_train: 80 images\n",
            "potty: 72 images\n",
            "mobile: 49 images\n",
            "plate: 64 images\n",
            "fork: 76 images\n",
            "fan: 70 images\n",
            "sofa: 85 images\n",
            "notebook: 72 images\n",
            "hat: 70 images\n",
            "bag: 81 images\n",
            "watering_can: 78 images\n",
            "broom: 73 images\n",
            "towel: 60 images\n",
            "apple: 71 images\n",
            "doll: 73 images\n",
            "vacuum: 61 images\n",
            "pillow: 95 images\n",
            "plastic_cup: 79 images\n",
            "sippy_cup: 64 images\n",
            "light: 66 images\n",
            "tissue_box: 74 images\n",
            "ball: 73 images\n",
            "marker: 63 images\n",
            "box: 62 images\n",
            "crib: 85 images\n",
            "window: 85 images\n",
            "keys: 74 images\n",
            "blanket: 74 images\n",
            "toy_phone: 69 images\n",
            "fridge_magnet: 75 images\n",
            "teddy_bear: 77 images\n",
            "lamp: 86 images\n",
            "milk: 47 images\n",
            "comb: 80 images\n",
            "trash_can: 72 images\n",
            "toothpaste: 75 images\n",
            "stuffed_animal: 75 images\n",
            "toilet_paper: 73 images\n",
            "puzzle: 67 images\n",
            "phone: 66 images\n",
            "brush: 82 images\n",
            "shirt: 76 images\n",
            "pacifier: 78 images\n",
            "diaper: 70 images\n",
            "toothbrush: 72 images\n",
            "pajamas: 83 images\n",
            "train: 52 images\n",
            "high_chair: 84 images\n",
            "chair: 83 images\n",
            "shampoo: 73 images\n",
            "bed: 86 images\n",
            "rocking_chair: 81 images\n",
            "blocks: 68 images\n",
            "clock: 65 images\n",
            "shoes: 73 images\n",
            "measuring_cup: 75 images\n",
            "table: 79 images\n",
            "toy_dinosaur: 80 images\n",
            "dustpan: 78 images\n",
            "toy_car: 82 images\n",
            "playpen: 72 images\n",
            "crayons: 79 images\n",
            "scarf: 1 images\n",
            "bottle: 70 images\n",
            "book: 75 images\n",
            "toy: 79 images\n",
            "drawer: 70 images\n",
            "carpet: 97 images\n",
            "toy_blocks: 72 images\n",
            "laundry_basket: 94 images\n",
            "balloon: 74 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "main_dataset = '/content/drive/MyDrive/Colab Notebooks/AIPI 540/Computer Vision Project/dataset'\n",
        "\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "if os.path.exists(main_dataset):\n",
        "    print('Path exists, conducting extraction of image data for processing. I decide to go the path of Gaussian NB using Historgrams')\n",
        "\n",
        "    for folder in os.listdir(main_dataset):\n",
        "        class_path = os.path.join(main_dataset, folder)\n",
        "\n",
        "        if os.path.isdir(class_path):\n",
        "            print(f'{class_path} added to processing queue')\n",
        "\n",
        "            for image_file in os.listdir(class_path):\n",
        "                if image_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                    image_path = os.path.join(class_path, image_file)\n",
        "                    file_paths.append(image_path)\n",
        "                    labels.append(folder)\n",
        "else:\n",
        "    print('Path does not exist - you need to try again')\n",
        "\n",
        "unique_classes = list(set(labels))\n",
        "print(f'Total classes: {len(unique_classes)}')\n",
        "print(f'Total images: {len(file_paths)}')\n",
        "print(f'Classes found: {unique_classes}')\n",
        "\n",
        "for class_name in unique_classes:\n",
        "    count = labels.count(class_name)\n",
        "    print(f'{class_name}: {count} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbU-KXcQitC7",
        "outputId": "52bbc040-45e0-43cd-b2eb-26fea5a3d6d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 1/6976 images\n",
            "Processed 51/6976 images\n",
            "Processed 101/6976 images\n",
            "Processed 151/6976 images\n",
            "Processed 201/6976 images\n",
            "Processed 251/6976 images\n",
            "Processed 301/6976 images\n",
            "Processed 351/6976 images\n",
            "Processed 401/6976 images\n",
            "Processed 451/6976 images\n",
            "Processed 501/6976 images\n",
            "Processed 551/6976 images\n",
            "Processed 601/6976 images\n",
            "Processed 651/6976 images\n",
            "Processed 701/6976 images\n",
            "Processed 751/6976 images\n",
            "Processed 801/6976 images\n",
            "Processed 851/6976 images\n",
            "Processed 901/6976 images\n",
            "Processed 951/6976 images\n",
            "Processed 1001/6976 images\n",
            "Processed 1051/6976 images\n",
            "Processed 1101/6976 images\n",
            "Processed 1151/6976 images\n",
            "Processed 1201/6976 images\n",
            "Processed 1251/6976 images\n",
            "Processed 1301/6976 images\n",
            "Processed 1351/6976 images\n",
            "Processed 1401/6976 images\n",
            "Processed 1451/6976 images\n",
            "Processed 1501/6976 images\n",
            "Processed 1551/6976 images\n",
            "Processed 1601/6976 images\n",
            "Processed 1651/6976 images\n",
            "Processed 1701/6976 images\n",
            "Processed 1751/6976 images\n",
            "Processed 1801/6976 images\n",
            "Processed 1851/6976 images\n",
            "Processed 1901/6976 images\n",
            "Processed 1951/6976 images\n",
            "Processed 2001/6976 images\n",
            "Processed 2051/6976 images\n",
            "Processed 2101/6976 images\n",
            "Processed 2151/6976 images\n",
            "Processed 2201/6976 images\n",
            "Processed 2251/6976 images\n",
            "Processed 2301/6976 images\n",
            "Processed 2351/6976 images\n",
            "Processed 2401/6976 images\n",
            "Processed 2451/6976 images\n",
            "Processed 2501/6976 images\n",
            "Processed 2551/6976 images\n",
            "Processed 2601/6976 images\n",
            "Processed 2651/6976 images\n",
            "Processed 2701/6976 images\n",
            "Processed 2751/6976 images\n",
            "Processed 2801/6976 images\n",
            "Processed 2851/6976 images\n",
            "Processed 2901/6976 images\n",
            "Processed 2951/6976 images\n",
            "Processed 3001/6976 images\n",
            "Processed 3051/6976 images\n",
            "Processed 3101/6976 images\n",
            "Processed 3151/6976 images\n",
            "Processed 3201/6976 images\n",
            "Processed 3251/6976 images\n",
            "Processed 3301/6976 images\n",
            "Processed 3351/6976 images\n",
            "Processed 3401/6976 images\n",
            "Processed 3451/6976 images\n",
            "Processed 3501/6976 images\n",
            "Processed 3551/6976 images\n",
            "Processed 3601/6976 images\n",
            "Processed 3651/6976 images\n",
            "Processed 3701/6976 images\n",
            "Processed 3751/6976 images\n",
            "Processed 3801/6976 images\n",
            "Processed 3851/6976 images\n",
            "Processed 3901/6976 images\n",
            "Processed 3951/6976 images\n",
            "Processed 4001/6976 images\n",
            "Processed 4051/6976 images\n",
            "Processed 4101/6976 images\n",
            "Processed 4151/6976 images\n",
            "Processed 4201/6976 images\n",
            "Processed 4251/6976 images\n",
            "Processed 4301/6976 images\n",
            "Processed 4351/6976 images\n",
            "Processed 4401/6976 images\n",
            "Processed 4451/6976 images\n",
            "Processed 4501/6976 images\n",
            "Processed 4551/6976 images\n",
            "Processed 4601/6976 images\n",
            "Processed 4651/6976 images\n",
            "Processed 4701/6976 images\n",
            "Processed 4751/6976 images\n",
            "Processed 4801/6976 images\n",
            "Processed 4851/6976 images\n",
            "Processed 4901/6976 images\n",
            "Processed 4951/6976 images\n",
            "Processed 5001/6976 images\n",
            "Processed 5051/6976 images\n",
            "Processed 5101/6976 images\n",
            "Processed 5151/6976 images\n",
            "Processed 5201/6976 images\n",
            "Processed 5251/6976 images\n",
            "Processed 5301/6976 images\n",
            "Processed 5351/6976 images\n",
            "Processed 5401/6976 images\n",
            "Processed 5451/6976 images\n",
            "Processed 5501/6976 images\n",
            "Processed 5551/6976 images\n",
            "Processed 5601/6976 images\n",
            "Processed 5651/6976 images\n",
            "Processed 5701/6976 images\n",
            "Processed 5751/6976 images\n",
            "Processed 5801/6976 images\n",
            "Processed 5851/6976 images\n",
            "Processed 5901/6976 images\n",
            "Processed 5951/6976 images\n",
            "Processed 6001/6976 images\n",
            "Processed 6051/6976 images\n",
            "Processed 6101/6976 images\n",
            "Processed 6151/6976 images\n",
            "Processed 6201/6976 images\n",
            "Processed 6251/6976 images\n",
            "Processed 6301/6976 images\n",
            "Processed 6351/6976 images\n",
            "Processed 6401/6976 images\n",
            "Processed 6451/6976 images\n",
            "Processed 6501/6976 images\n",
            "Processed 6551/6976 images\n",
            "Processed 6601/6976 images\n",
            "Processed 6651/6976 images\n",
            "Processed 6701/6976 images\n",
            "Processed 6751/6976 images\n",
            "Processed 6801/6976 images\n",
            "Processed 6851/6976 images\n",
            "Processed 6901/6976 images\n",
            "Processed 6951/6976 images\n",
            "Feature extraction complete!\n",
            "Feature matrix shape: (6976, 96)\n",
            "Total samples: 6976\n",
            "\n",
            "Step 3: Training and evaluating traditional ML models...\n",
            "GAUSSIAN NB:\n",
            "Accuracy: 0.0659\n",
            "Confusion Matrix:\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 3 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "         apple       0.17      0.05      0.08        19\n",
            "      backpack       0.12      0.06      0.08        18\n",
            "           bag       0.00      0.00      0.00        21\n",
            "          ball       0.00      0.00      0.00        12\n",
            "       balloon       0.00      0.00      0.00        14\n",
            "        banana       0.00      0.00      0.00        15\n",
            "           bed       0.12      0.06      0.08        16\n",
            "           bib       0.04      0.12      0.05        16\n",
            "       blanket       0.00      0.00      0.00        18\n",
            "        blocks       0.09      0.07      0.08        15\n",
            "          book       0.00      0.00      0.00        15\n",
            "     booklight       0.00      0.00      0.00        12\n",
            "        bottle       0.03      0.18      0.05        11\n",
            "          bowl       0.00      0.00      0.00        21\n",
            "           box       0.25      0.25      0.25         8\n",
            "         broom       0.00      0.00      0.00        18\n",
            "         brush       0.00      0.00      0.00        19\n",
            "        carpet       0.10      0.21      0.13        19\n",
            "         chair       0.00      0.00      0.00        22\n",
            "         clock       0.00      0.00      0.00         6\n",
            "          coat       0.00      0.00      0.00        12\n",
            "          comb       0.11      0.06      0.08        16\n",
            "       crayons       0.17      0.10      0.12        21\n",
            "          crib       0.05      0.27      0.08        11\n",
            "           cup       0.00      0.00      0.00        11\n",
            "       curtain       0.09      0.26      0.13        23\n",
            "        diaper       0.00      0.00      0.00        11\n",
            "          doll       0.00      0.00      0.00        14\n",
            "          door       0.57      0.25      0.35        16\n",
            "        drawer       0.00      0.00      0.00        13\n",
            "       dustpan       0.00      0.00      0.00        11\n",
            "           fan       0.00      0.00      0.00        20\n",
            "     floor_mat       0.06      0.14      0.09        14\n",
            "          fork       0.13      0.68      0.21        19\n",
            " fridge_magnet       0.00      0.00      0.00        14\n",
            "        gloves       0.00      0.00      0.00        10\n",
            "           hat       0.00      0.00      0.00        12\n",
            "    high_chair       0.00      0.00      0.00        14\n",
            "         juice       0.25      0.35      0.29        20\n",
            "          keys       0.00      0.00      0.00        19\n",
            "          lamp       0.00      0.00      0.00        13\n",
            "laundry_basket       0.00      0.00      0.00        11\n",
            "         light       0.16      0.31      0.21        13\n",
            "        marker       0.09      0.24      0.12        17\n",
            " measuring_cup       0.00      0.00      0.00        19\n",
            "          milk       0.00      0.00      0.00         9\n",
            "        mirror       0.00      0.00      0.00         9\n",
            "        mobile       0.00      0.00      0.00         8\n",
            "    nightlight       0.00      0.00      0.00         7\n",
            "      notebook       0.00      0.00      0.00         9\n",
            "      pacifier       0.00      0.00      0.00        22\n",
            "       pajamas       0.00      0.00      0.00        16\n",
            "         pants       0.00      0.00      0.00        10\n",
            "           pen       0.00      0.00      0.00        11\n",
            "         phone       0.00      0.00      0.00        11\n",
            "        pillow       0.20      0.05      0.08        19\n",
            "   plastic_cup       0.00      0.00      0.00        26\n",
            "         plate       0.00      0.00      0.00        11\n",
            "       playpen       0.00      0.00      0.00        21\n",
            "         potty       0.11      0.10      0.11        10\n",
            "        puzzle       0.00      0.00      0.00        13\n",
            "        rattle       0.00      0.00      0.00        14\n",
            "        remote       0.21      0.23      0.22        13\n",
            " rocking_chair       0.02      0.05      0.03        19\n",
            "   rubber_duck       0.12      0.09      0.11        11\n",
            "         scarf       0.00      0.00      0.00         1\n",
            "       shampoo       0.00      0.00      0.00        13\n",
            "         shirt       0.00      0.00      0.00        18\n",
            "         shoes       0.00      0.00      0.00        20\n",
            "     sippy_cup       0.03      0.38      0.05         8\n",
            "          soap       0.00      0.00      0.00        22\n",
            "          sock       0.33      0.06      0.11        16\n",
            "          sofa       0.06      0.19      0.09        16\n",
            "         spoon       0.00      0.00      0.00        18\n",
            "stuffed_animal       0.00      0.00      0.00        15\n",
            "         table       0.00      0.00      0.00        19\n",
            "    teddy_bear       0.00      0.00      0.00        11\n",
            "    tissue_box       0.00      0.00      0.00        16\n",
            "  toilet_paper       0.00      0.00      0.00        13\n",
            "    toothbrush       0.00      0.00      0.00        19\n",
            "    toothpaste       0.00      0.00      0.00        19\n",
            "         towel       0.00      0.00      0.00        10\n",
            "           toy       0.00      0.00      0.00        12\n",
            "    toy_blocks       0.06      0.07      0.06        15\n",
            "       toy_car       0.00      0.00      0.00        15\n",
            "  toy_dinosaur       0.06      0.58      0.11        19\n",
            "     toy_phone       0.00      0.00      0.00         9\n",
            "toy_ring_stack       0.00      0.00      0.00        17\n",
            "     toy_train       0.00      0.00      0.00        18\n",
            "         train       0.05      0.71      0.10         7\n",
            "     trash_can       0.33      0.08      0.12        13\n",
            "        vacuum       0.00      0.00      0.00        16\n",
            "         water       0.00      0.00      0.00        11\n",
            "  watering_can       0.00      0.00      0.00        16\n",
            "        window       0.00      0.00      0.00        16\n",
            "\n",
            "      accuracy                           0.07      1396\n",
            "     macro avg       0.04      0.07      0.04      1396\n",
            "  weighted avg       0.05      0.07      0.04      1396\n",
            "\n",
            "SVC(I had wanted to perform GridSearch but processessing took too long)\n",
            "Accuracy: 0.0759\n",
            "Confusion Matrix:\n",
            "[[0 1 0 ... 1 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 5 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 5 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "         apple       0.00      0.00      0.00        19\n",
            "      backpack       0.05      0.06      0.05        18\n",
            "           bag       0.11      0.05      0.07        21\n",
            "          ball       0.20      0.08      0.12        12\n",
            "       balloon       0.00      0.00      0.00        14\n",
            "        banana       0.29      0.13      0.18        15\n",
            "           bed       0.02      0.06      0.02        16\n",
            "           bib       0.00      0.00      0.00        16\n",
            "       blanket       0.00      0.00      0.00        18\n",
            "        blocks       0.09      0.40      0.15        15\n",
            "          book       0.06      0.07      0.06        15\n",
            "     booklight       0.00      0.00      0.00        12\n",
            "        bottle       0.00      0.00      0.00        11\n",
            "          bowl       0.00      0.00      0.00        21\n",
            "           box       0.25      0.12      0.17         8\n",
            "         broom       0.00      0.00      0.00        18\n",
            "         brush       0.07      0.47      0.12        19\n",
            "        carpet       0.13      0.37      0.19        19\n",
            "         chair       0.00      0.00      0.00        22\n",
            "         clock       0.00      0.00      0.00         6\n",
            "          coat       0.00      0.00      0.00        12\n",
            "          comb       0.25      0.06      0.10        16\n",
            "       crayons       0.33      0.05      0.08        21\n",
            "          crib       0.03      0.36      0.06        11\n",
            "           cup       0.11      0.18      0.13        11\n",
            "       curtain       0.00      0.00      0.00        23\n",
            "        diaper       0.00      0.00      0.00        11\n",
            "          doll       0.00      0.00      0.00        14\n",
            "          door       0.00      0.00      0.00        16\n",
            "        drawer       0.00      0.00      0.00        13\n",
            "       dustpan       0.00      0.00      0.00        11\n",
            "           fan       0.00      0.00      0.00        20\n",
            "     floor_mat       0.01      0.14      0.02        14\n",
            "          fork       0.18      0.63      0.28        19\n",
            " fridge_magnet       0.00      0.00      0.00        14\n",
            "        gloves       0.04      0.10      0.06        10\n",
            "           hat       0.00      0.00      0.00        12\n",
            "    high_chair       0.00      0.00      0.00        14\n",
            "         juice       0.32      0.50      0.39        20\n",
            "          keys       0.00      0.00      0.00        19\n",
            "          lamp       0.00      0.00      0.00        13\n",
            "laundry_basket       0.02      0.09      0.03        11\n",
            "         light       0.30      0.23      0.26        13\n",
            "        marker       0.00      0.00      0.00        17\n",
            " measuring_cup       0.00      0.00      0.00        19\n",
            "          milk       0.00      0.00      0.00         9\n",
            "        mirror       0.00      0.00      0.00         9\n",
            "        mobile       0.00      0.00      0.00         8\n",
            "    nightlight       0.00      0.00      0.00         7\n",
            "      notebook       0.09      0.11      0.10         9\n",
            "      pacifier       0.00      0.00      0.00        22\n",
            "       pajamas       0.29      0.38      0.32        16\n",
            "         pants       0.00      0.00      0.00        10\n",
            "           pen       0.00      0.00      0.00        11\n",
            "         phone       0.00      0.00      0.00        11\n",
            "        pillow       0.00      0.00      0.00        19\n",
            "   plastic_cup       0.13      0.08      0.10        26\n",
            "         plate       1.00      0.09      0.17        11\n",
            "       playpen       0.00      0.00      0.00        21\n",
            "         potty       0.00      0.00      0.00        10\n",
            "        puzzle       0.00      0.00      0.00        13\n",
            "        rattle       0.00      0.00      0.00        14\n",
            "        remote       0.00      0.00      0.00        13\n",
            " rocking_chair       0.00      0.00      0.00        19\n",
            "   rubber_duck       0.43      0.27      0.33        11\n",
            "         scarf       0.00      0.00      0.00         1\n",
            "       shampoo       0.00      0.00      0.00        13\n",
            "         shirt       0.11      0.11      0.11        18\n",
            "         shoes       0.00      0.00      0.00        20\n",
            "     sippy_cup       0.04      0.12      0.06         8\n",
            "          soap       0.00      0.00      0.00        22\n",
            "          sock       0.09      0.06      0.07        16\n",
            "          sofa       0.08      0.44      0.13        16\n",
            "         spoon       0.33      0.06      0.10        18\n",
            "stuffed_animal       0.00      0.00      0.00        15\n",
            "         table       0.00      0.00      0.00        19\n",
            "    teddy_bear       0.00      0.00      0.00        11\n",
            "    tissue_box       0.00      0.00      0.00        16\n",
            "  toilet_paper       0.00      0.00      0.00        13\n",
            "    toothbrush       0.00      0.00      0.00        19\n",
            "    toothpaste       0.00      0.00      0.00        19\n",
            "         towel       0.00      0.00      0.00        10\n",
            "           toy       0.00      0.00      0.00        12\n",
            "    toy_blocks       0.12      0.07      0.09        15\n",
            "       toy_car       0.00      0.00      0.00        15\n",
            "  toy_dinosaur       0.04      0.26      0.07        19\n",
            "     toy_phone       0.04      0.11      0.05         9\n",
            "toy_ring_stack       0.00      0.00      0.00        17\n",
            "     toy_train       0.00      0.00      0.00        18\n",
            "         train       0.00      0.00      0.00         7\n",
            "     trash_can       0.20      0.15      0.17        13\n",
            "        vacuum       0.00      0.00      0.00        16\n",
            "         water       0.21      0.45      0.29        11\n",
            "  watering_can       0.00      0.00      0.00        16\n",
            "        window       0.00      0.00      0.00        16\n",
            "\n",
            "      accuracy                           0.08      1396\n",
            "     macro avg       0.06      0.07      0.05      1396\n",
            "  weighted avg       0.06      0.08      0.05      1396\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def extract_color_histogram(image_path, bins=32):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((128, 128))\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    image_array = np.array(image)\n",
        "\n",
        "    hist_r = np.histogram(image_array[:,:,0], bins=bins, range=(0, 256))[0]\n",
        "    hist_g = np.histogram(image_array[:,:,1], bins=bins, range=(0, 256))[0]\n",
        "    hist_b = np.histogram(image_array[:,:,2], bins=bins, range=(0, 256))[0]\n",
        "\n",
        "    hist_r = hist_r / np.sum(hist_r)\n",
        "    hist_g = hist_g / np.sum(hist_g)\n",
        "    hist_b = hist_b / np.sum(hist_b)\n",
        "\n",
        "    color_features = np.concatenate([hist_r, hist_g, hist_b])\n",
        "    return color_features\n",
        "\n",
        "features = []\n",
        "processed_labels = []\n",
        "\n",
        "for i, image_path in enumerate(file_paths):\n",
        "    try:\n",
        "        color_hist = extract_color_histogram(image_path, bins=32)\n",
        "        features.append(color_hist)\n",
        "        processed_labels.append(labels[i])\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Processed {i+1}/{len(file_paths)} images\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing.') {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "X = np.array(features)\n",
        "y = np.array(processed_labels)\n",
        "\n",
        "print(f\"Feature extraction complete!\")\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Total samples: {len(y)}\")\n",
        "\n",
        "\n",
        "print(\" Training and evaluating traditional ML models\")\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"GAUSSIAN NB:\")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "gnb_pred = gnb.predict(X_test)\n",
        "gnb_accuracy = accuracy_score(y_test, gnb_pred)\n",
        "\n",
        "print(f\"Accuracy: {gnb_accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, gnb_pred))\n",
        "print(\"Classification Report:\")\n",
        "unique_test_labels = sorted(list(set(y_test)))\n",
        "test_class_names = [le.classes_[i] for i in unique_test_labels]\n",
        "print(classification_report(y_test, gnb_pred, labels=unique_test_labels, target_names=test_class_names, zero_division=0))\n",
        "\n",
        "print(\"SVC(I had wanted to perform GridSearch but processessing took too long)\")\n",
        "\n",
        "svc = SVC(kernel='rbf', random_state=42)\n",
        "svc.fit(X_train, y_train)\n",
        "svc_pred = svc.predict(X_test)\n",
        "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
        "\n",
        "print(f\"Accuracy: {svc_accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, svc_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, svc_pred, labels=unique_test_labels, target_names=test_class_names, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwY7PNmgi1ha",
        "outputId": "3160fdec-27ac-4685-d808-864ec0afe1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 209MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ResNet50...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 4.5445\n",
            "Epoch 2/100, Loss: 4.1203\n",
            "Epoch 3/100, Loss: 3.7590\n",
            "Epoch 4/100, Loss: 3.4368\n",
            "Epoch 5/100, Loss: 3.0938\n",
            "Epoch 6/100, Loss: 2.8130\n",
            "Epoch 7/100, Loss: 2.5087\n",
            "Epoch 8/100, Loss: 2.2530\n",
            "Epoch 9/100, Loss: 1.9729\n",
            "Epoch 10/100, Loss: 1.7319\n",
            "Epoch 11/100, Loss: 1.4639\n",
            "Epoch 12/100, Loss: 1.1830\n",
            "Epoch 13/100, Loss: 0.9427\n",
            "Epoch 14/100, Loss: 0.7382\n",
            "Epoch 15/100, Loss: 0.5581\n",
            "Epoch 16/100, Loss: 0.4194\n",
            "Epoch 17/100, Loss: 0.3158\n",
            "Epoch 18/100, Loss: 0.2543\n",
            "Epoch 19/100, Loss: 0.2319\n",
            "Epoch 20/100, Loss: 0.2367\n",
            "Epoch 21/100, Loss: 0.2248\n",
            "Epoch 22/100, Loss: 0.2087\n",
            "Epoch 23/100, Loss: 0.1672\n",
            "Epoch 24/100, Loss: 0.1788\n",
            "Epoch 25/100, Loss: 0.1481\n",
            "Epoch 26/100, Loss: 0.1251\n",
            "Epoch 27/100, Loss: 0.1530\n",
            "Epoch 28/100, Loss: 0.1366\n",
            "Epoch 29/100, Loss: 0.1583\n",
            "Epoch 30/100, Loss: 0.1668\n",
            "Epoch 31/100, Loss: 0.1233\n",
            "Epoch 32/100, Loss: 0.1266\n",
            "Epoch 33/100, Loss: 0.1299\n",
            "Epoch 34/100, Loss: 0.1716\n",
            "Epoch 35/100, Loss: 0.1440\n",
            "Epoch 36/100, Loss: 0.0739\n",
            "Epoch 37/100, Loss: 0.0625\n",
            "Epoch 38/100, Loss: 0.0552\n",
            "Epoch 39/100, Loss: 0.0984\n",
            "Epoch 40/100, Loss: 0.1233\n",
            "Epoch 41/100, Loss: 0.1378\n",
            "Epoch 42/100, Loss: 0.1535\n",
            "Epoch 43/100, Loss: 0.0952\n",
            "Epoch 44/100, Loss: 0.0834\n",
            "Epoch 45/100, Loss: 0.0524\n",
            "Epoch 46/100, Loss: 0.0377\n",
            "Epoch 47/100, Loss: 0.1065\n",
            "Epoch 48/100, Loss: 0.1042\n",
            "Epoch 49/100, Loss: 0.1127\n",
            "Epoch 50/100, Loss: 0.1391\n",
            "Epoch 51/100, Loss: 0.1405\n",
            "Epoch 52/100, Loss: 0.0879\n",
            "Epoch 53/100, Loss: 0.0446\n",
            "Epoch 54/100, Loss: 0.0321\n",
            "Epoch 55/100, Loss: 0.0299\n",
            "Epoch 56/100, Loss: 0.0229\n",
            "Epoch 57/100, Loss: 0.0271\n",
            "Epoch 58/100, Loss: 0.0248\n",
            "Epoch 59/100, Loss: 0.0201\n",
            "Epoch 60/100, Loss: 0.0202\n",
            "Epoch 61/100, Loss: 0.4306\n",
            "Epoch 62/100, Loss: 0.1982\n",
            "Epoch 63/100, Loss: 0.0590\n",
            "Epoch 64/100, Loss: 0.0453\n",
            "Epoch 65/100, Loss: 0.0308\n",
            "Epoch 66/100, Loss: 0.0255\n",
            "Epoch 67/100, Loss: 0.0253\n",
            "Epoch 68/100, Loss: 0.0233\n",
            "Epoch 69/100, Loss: 0.0272\n",
            "Epoch 70/100, Loss: 0.0488\n",
            "Epoch 71/100, Loss: 0.2834\n",
            "Epoch 72/100, Loss: 0.0988\n",
            "Epoch 73/100, Loss: 0.0420\n",
            "Epoch 74/100, Loss: 0.0242\n",
            "Epoch 75/100, Loss: 0.0246\n",
            "Epoch 76/100, Loss: 0.0211\n",
            "Epoch 77/100, Loss: 0.0184\n",
            "Epoch 78/100, Loss: 0.0157\n",
            "Epoch 79/100, Loss: 0.0155\n",
            "Epoch 80/100, Loss: 0.0154\n",
            "Epoch 81/100, Loss: 0.0148\n",
            "Epoch 82/100, Loss: 0.0149\n",
            "Epoch 83/100, Loss: 0.0135\n",
            "Epoch 84/100, Loss: 0.0138\n",
            "Epoch 85/100, Loss: 0.0153\n",
            "Epoch 86/100, Loss: 0.5287\n",
            "Epoch 87/100, Loss: 0.1155\n",
            "Epoch 88/100, Loss: 0.0392\n",
            "Epoch 89/100, Loss: 0.0257\n",
            "Epoch 90/100, Loss: 0.0167\n",
            "Epoch 91/100, Loss: 0.0162\n",
            "Epoch 92/100, Loss: 0.0157\n",
            "Epoch 93/100, Loss: 0.0154\n",
            "Epoch 94/100, Loss: 0.0150\n",
            "Epoch 95/100, Loss: 0.0165\n",
            "Epoch 96/100, Loss: 0.0175\n",
            "Epoch 97/100, Loss: 0.1009\n",
            "Epoch 98/100, Loss: 0.3157\n",
            "Epoch 99/100, Loss: 0.0497\n",
            "Epoch 100/100, Loss: 0.0274\n",
            "Accuracy: 0.4785\n",
            "Confusion Matrix:\n",
            "[[ 9  0  1 ...  1  1  0]\n",
            " [ 0  8  0 ...  0  0  0]\n",
            " [ 0  2  8 ...  0  1  0]\n",
            " ...\n",
            " [ 0  0  0 ... 10  0  0]\n",
            " [ 1  0  0 ...  0  9  0]\n",
            " [ 0  0  0 ...  0  0 15]]\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "         apple       0.64      0.47      0.55        19\n",
            "      backpack       0.67      0.44      0.53        18\n",
            "           bag       0.53      0.38      0.44        21\n",
            "          ball       0.38      0.50      0.43        12\n",
            "       balloon       0.57      0.29      0.38        14\n",
            "        banana       0.50      0.33      0.40        15\n",
            "           bed       0.50      0.62      0.56        16\n",
            "           bib       0.59      0.62      0.61        16\n",
            "       blanket       0.31      0.22      0.26        18\n",
            "        blocks       0.25      0.33      0.29        15\n",
            "          book       0.43      0.40      0.41        15\n",
            "     booklight       0.24      0.33      0.28        12\n",
            "        bottle       0.56      0.45      0.50        11\n",
            "          bowl       0.76      0.62      0.68        21\n",
            "           box       0.67      0.75      0.71         8\n",
            "         broom       0.67      0.56      0.61        18\n",
            "         brush       0.59      0.53      0.56        19\n",
            "        carpet       0.53      0.53      0.53        19\n",
            "         chair       0.85      0.50      0.63        22\n",
            "         clock       0.57      0.67      0.62         6\n",
            "          coat       0.50      0.42      0.45        12\n",
            "          comb       0.73      0.50      0.59        16\n",
            "       crayons       0.50      0.43      0.46        21\n",
            "          crib       0.80      0.73      0.76        11\n",
            "           cup       0.50      0.18      0.27        11\n",
            "       curtain       0.67      0.78      0.72        23\n",
            "        diaper       0.08      0.09      0.09        11\n",
            "          doll       0.40      0.43      0.41        14\n",
            "          door       0.67      0.75      0.71        16\n",
            "        drawer       0.57      0.31      0.40        13\n",
            "       dustpan       0.26      0.64      0.37        11\n",
            "           fan       0.41      0.35      0.38        20\n",
            "     floor_mat       0.26      0.50      0.34        14\n",
            "          fork       0.52      0.58      0.55        19\n",
            " fridge_magnet       0.54      0.50      0.52        14\n",
            "        gloves       0.36      0.40      0.38        10\n",
            "           hat       0.33      0.67      0.44        12\n",
            "    high_chair       0.62      0.71      0.67        14\n",
            "         juice       0.75      0.60      0.67        20\n",
            "          keys       0.60      0.47      0.53        19\n",
            "          lamp       0.20      0.08      0.11        13\n",
            "laundry_basket       0.33      0.64      0.44        11\n",
            "         light       0.35      0.54      0.42        13\n",
            "        marker       0.38      0.18      0.24        17\n",
            " measuring_cup       0.62      0.26      0.37        19\n",
            "          milk       0.38      0.33      0.35         9\n",
            "        mirror       0.25      0.33      0.29         9\n",
            "        mobile       0.20      0.38      0.26         8\n",
            "    nightlight       0.12      0.14      0.13         7\n",
            "      notebook       0.44      0.44      0.44         9\n",
            "      pacifier       0.60      0.55      0.57        22\n",
            "       pajamas       0.39      0.56      0.46        16\n",
            "         pants       0.73      0.80      0.76        10\n",
            "           pen       0.45      0.91      0.61        11\n",
            "         phone       0.40      0.18      0.25        11\n",
            "        pillow       0.52      0.68      0.59        19\n",
            "   plastic_cup       0.67      0.62      0.64        26\n",
            "         plate       0.78      0.64      0.70        11\n",
            "       playpen       0.41      0.57      0.48        21\n",
            "         potty       0.33      0.50      0.40        10\n",
            "        puzzle       0.23      0.38      0.29        13\n",
            "        rattle       0.12      0.14      0.13        14\n",
            "        remote       0.75      0.69      0.72        13\n",
            " rocking_chair       0.52      0.58      0.55        19\n",
            "   rubber_duck       0.39      0.64      0.48        11\n",
            "         scarf       0.00      0.00      0.00         1\n",
            "       shampoo       0.50      0.69      0.58        13\n",
            "         shirt       0.71      0.56      0.62        18\n",
            "         shoes       0.69      0.45      0.55        20\n",
            "     sippy_cup       0.29      0.75      0.41         8\n",
            "          soap       0.30      0.14      0.19        22\n",
            "          sock       0.68      0.81      0.74        16\n",
            "          sofa       0.62      0.50      0.55        16\n",
            "         spoon       0.64      0.50      0.56        18\n",
            "stuffed_animal       0.29      0.27      0.28        15\n",
            "         table       0.86      0.63      0.73        19\n",
            "    teddy_bear       0.43      0.27      0.33        11\n",
            "    tissue_box       0.60      0.38      0.46        16\n",
            "  toilet_paper       0.18      0.23      0.20        13\n",
            "    toothbrush       0.50      0.37      0.42        19\n",
            "    toothpaste       0.70      0.37      0.48        19\n",
            "         towel       0.27      0.40      0.32        10\n",
            "           toy       0.10      0.08      0.09        12\n",
            "    toy_blocks       0.06      0.07      0.06        15\n",
            "       toy_car       0.36      0.67      0.47        15\n",
            "  toy_dinosaur       0.70      0.37      0.48        19\n",
            "     toy_phone       0.33      0.33      0.33         9\n",
            "toy_ring_stack       0.78      0.82      0.80        17\n",
            "     toy_train       0.50      0.22      0.31        18\n",
            "         train       0.67      0.57      0.62         7\n",
            "     trash_can       0.33      0.46      0.39        13\n",
            "        vacuum       0.38      0.19      0.25        16\n",
            "         water       0.53      0.91      0.67        11\n",
            "  watering_can       0.50      0.56      0.53        16\n",
            "        window       0.88      0.94      0.91        16\n",
            "\n",
            "      accuracy                           0.48      1396\n",
            "     macro avg       0.48      0.47      0.46      1396\n",
            "  weighted avg       0.51      0.48      0.48      1396\n",
            "\n",
            "Model Comparisn:\n",
            "Gaussian Naive Bayes: 0.0659\n",
            "Support Vector Classifier: 0.0759\n",
            "ResNet50 (PyTorch): 0.4785\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels[idx]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    file_paths, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ImageDataset(train_paths, train_labels, transform)\n",
        "test_dataset = ImageDataset(test_paths, test_labels, transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(le.classes_))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Training ResNet50...\")\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/100, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "model.eval()\n",
        "resnet_pred = []\n",
        "resnet_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        resnet_pred.extend(predicted.cpu().numpy())\n",
        "        resnet_true.extend(labels.cpu().numpy())\n",
        "\n",
        "resnet_accuracy = accuracy_score(resnet_true, resnet_pred)\n",
        "\n",
        "print(f\"Accuracy: {resnet_accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(resnet_true, resnet_pred))\n",
        "print(\"Classification Report:\")\n",
        "resnet_unique_labels = sorted(list(set(resnet_true)))\n",
        "resnet_class_names = [le.classes_[i] for i in resnet_unique_labels]\n",
        "print(classification_report(resnet_true, resnet_pred, labels=resnet_unique_labels, target_names=resnet_class_names, zero_division=0))\n",
        "\n",
        "\n",
        "print(\"Model Comparisn:\")\n",
        "\n",
        "print(f\"Gaussian Naive Bayes: {gnb_accuracy:.4f}\")\n",
        "print(f\"Support Vector Classifier: {svc_accuracy:.4f}\")\n",
        "print(f\"ResNet50 (PyTorch): {resnet_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOSWzgJcT5sI2CYKhkYcsDI"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}